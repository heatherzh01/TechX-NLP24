{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk1UKaNvrMv"
      },
      "source": [
        "## Introduction\n",
        "[PyTorch](https://pytorch.org/) is a machine learning framework that is used in both academia and industry for various applications. PyTorch started of as a more flexible alternative to [TensorFlow](https://www.tensorflow.org/), which is another popular machine learning framework. At the time of its release, `PyTorch` appealed to the users due to its user friendly nature: as opposed to defining static graphs before performing an operation as in `TensorFlow`, `PyTorch` allowed users to define their operations as they go, which is also the approached integrated by `TensorFlow` in its following releases. Although `TensorFlow` is more widely preferred in the industry, `PyTorch` is often times the preferred machine learning framework for researchers. If you would like to learn more about the differences between the two, you can check out [this](https://blog.udacity.com/2020/05/pytorch-vs-tensorflow-what-you-need-to-know.html) blog post.\n",
        "\n",
        "Now that we have learned enough about the background of `PyTorch`, let's start by importing it into our notebook. To install `PyTorch`, you can follow the instructions here. Alternatively, you can open this notebook using `Google Colab`, which already has `PyTorch` installed in its base kernel. Once you are done with the installation process, run the following cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adapted from the following resources:\n",
        "* \"Word Window Classification\" tutorial notebook by Matt Lamm, from Winter 2020 offering of CS224N\n",
        "* Official PyTorch Documentation on Deep Learning with PyTorch: A 60 Minute Blitz by Soumith Chintala\n",
        "* Stanford CS224N PyTorch Tutorial\n",
        "* Cornell Tech CS5787 PyTorch Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/hjz/Documents/GitHub/NovoObjectiveHandle/.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m575.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m768.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: networkx, torch\n",
            "Successfully installed networkx-3.3 torch-2.2.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u0ukr7quvrMx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFpb7B5miYE",
        "outputId": "c69c10b9-91f9-47d0-9a86-b251c5f4ba2c"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10ZRdcBwDP3"
      },
      "source": [
        "We are all set to start our tutorial. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdSN9ZXvrM0"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Tensors are the most basic building blocks in `PyTorch`.  Tensors are similar to matrices, but the have extra properties and they can represent higher dimensions. For example, an square image with 256 pixels in both sides can be represented by a `3x256x256` tensor, where the first 3 dimensions represent the color channels, red, green and blue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aWvTEy7lgG"
      },
      "source": [
        "### Tensor Initialization\n",
        "There are several ways to instantiate tensors in `PyTorch`, which we will go through next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78_3AMEyvJd"
      },
      "source": [
        "#### **From a Python List**\n",
        "\n",
        "We can initalize a tensor from a `Python` list, which could include sublists. The dimensions and the data types will be automatically inferred by `PyTorch` when we use [`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsjIW9I_ztiO",
        "outputId": "e8529742-c5c8-460d-955b-a0315cea7664"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor from a Python List\n",
        "data = [\n",
        "        [0, 1],\n",
        "        [2, 3],\n",
        "        [4, 5]\n",
        "       ]\n",
        "x_python = torch.tensor(data)\n",
        "\n",
        "# Print the tensor\n",
        "x_python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRSo1pnenKj8",
        "outputId": "fe8a9de5-4dbe-4fd8-ab2e-3514db9d29d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_python.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv6ZEoZ0RWb5"
      },
      "source": [
        "We can also call `torch.tensor()` with the optional `dtype` parameter, which will set the data type. Some useful datatypes to be familiar with are: `torch.bool`, `torch.float`, and `torch.long`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bQF5IhsD7-n",
        "outputId": "e0e7b6ba-9338-49bc-c70a-b8984869d741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are using the dtype to create a tensor of particular type\n",
        "x_float = torch.tensor(data, dtype=torch.float)\n",
        "x_float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16VoILaaE-_j",
        "outputId": "32bdf00c-f109-4f9b-ddf2-d5de30045f94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[False,  True],\n",
              "        [ True,  True],\n",
              "        [ True,  True]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are using the dtype to create a tensor of particular type\n",
        "x_bool = torch.tensor(data, dtype=torch.bool)\n",
        "x_bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOcOn8P9xw3W",
        "outputId": "5cf1e8cc-f149-49d3-9dcf-7a2c7a5737db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Can we multiply these?\n",
        "x_bool * x_float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mlUFIGon5Xk",
        "outputId": "0973018b-298c-4186-d3a7-f684062b5321"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_python.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4HccPWFEQUB"
      },
      "source": [
        "We can also get the same tensor in our specified data type using methods such as `float()`, `long()` etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_yq0SuTS_W",
        "outputId": "00948f72-6a9b-4f7f-c364-99e28f556106"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_python.double().dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFiS1OFdTlKE"
      },
      "source": [
        "We can also use `tensor.FloatTensor`, `tensor.LongTensor`, `tensor.Tensor` classes to instantiate a tensor of particular type. `LongTensor`s are particularly important in NLP as many methods that deal with indices require the indices to be passed as a `LongTensor`, which is a 64 bit integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXXWZ1H2TkNN",
        "outputId": "87b80c33-9749-4e4c-f60c-cb4b371c3fde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# `torch.Tensor` defaults to float\n",
        "# Same as torch.FloatTensor(data)\n",
        "x = torch.Tensor(data)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuLzDzsoytM2"
      },
      "source": [
        "#### **From a NumPy Array**\n",
        "We can also initialize a tensor from a `NumPy` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtSNe8X-2Pox",
        "outputId": "d7598eb2-8780-470e-bf80-ac5a0744e1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize a tensor from a NumPy array\n",
        "ndarray = np.array(data)\n",
        "x_numpy = torch.from_numpy(ndarray)\n",
        "\n",
        "# Print the tensor\n",
        "x_numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhtcBgum3OZ3"
      },
      "source": [
        "#### **From a Tensor**\n",
        "We can also initialize a tensor from another tensor, using the following methods:\n",
        "\n",
        "* `torch.ones_like(old_tensor)`: Initializes a tensor of `1s`.\n",
        "* `torch.zeros_like(old_tensor)`: Initializes a tensor of `0s`.\n",
        "* `torch.rand_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a uniform distribution between `0` and `1`.\n",
        "* `torch.randn_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a normal distribution.\n",
        "\n",
        "All of these methods preserve the tensor properties of the original tensor passed in, such as the `shape` and `device`, which we will cover in a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoKVhcLh2yqe",
        "outputId": "1a636cd9-3d50-4bff-f92c-0db255bba8ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a base tensor\n",
        "x = torch.tensor([[1., 2], [3, 4]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FncfGN6z7ELA",
        "outputId": "f295e39f-fb04-4dc4-e78e-18f5b6d9e740"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor of 0s\n",
        "x_zeros = torch.zeros_like(x)\n",
        "x_zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D993dpnP6iA8",
        "outputId": "ceb8ab81-c55f-44d4-92e3-7635fc442d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor of 1s\n",
        "x_ones = torch.ones_like(x)\n",
        "x_ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBUDeEm97IqW",
        "outputId": "ba45c695-0323-47a6-872e-3f7dbdfa4de7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7780, 0.7595],\n",
              "        [0.8201, 0.6796]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor where each element is sampled from a uniform distribution\n",
        "# between 0 and 1\n",
        "x_rand = torch.rand_like(x)\n",
        "x_rand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYsE3lKt7IEX",
        "outputId": "5424c6c7-dabc-49b4-965c-b1cdca507efa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7930,  1.7701],\n",
              "        [-1.1557,  0.7682]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor where each element is sampled from a normal distribution\n",
        "x_randn = torch.randn_like(x)\n",
        "x_randn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6tqf7v38vbi"
      },
      "source": [
        "#### **By Specifying a Shape**\n",
        "We can also instantiate tensors by specifying their shapes (which we will cover in more detail in a bit). The methods we could use follow the ones in the previous section:\n",
        "* `torch.zeros()`\n",
        "* `torch.ones()`\n",
        "* `torch.rand()`\n",
        "* `torch.randn()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh4I4Npz-dZ4",
        "outputId": "b8c86246-debb-4509-a7ed-5cdc1577d741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 2x3x2 tensor of 0s\n",
        "shape = (4, 2, 2)\n",
        "x_zeros = torch.zeros(shape) # x_zeros = torch.zeros(4, 3, 2) is an alternative\n",
        "x_zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEjeR24MLkN"
      },
      "source": [
        "#### **With `torch.arange()`**\n",
        "We can also create a tensor with `torch.arange(end)`, which returns a `1-D` tensor with elements ranging from `0` to `end-1`. We can use the optional `start` and `step` parameters to create tensors with different ranges.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EjARl2aM7pA",
        "outputId": "141b97a8-c203-4f69-960b-e2d2f3ca64f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor with values 0-9\n",
        "x = torch.arange(10)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgpkRn527zSr"
      },
      "source": [
        "### Tensor Properties\n",
        "\n",
        "Tensors have a few properties that are important for us to cover. These are namely `shape`, and the `device` properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBt6e4xZT3zr"
      },
      "source": [
        "#### Data Type\n",
        "\n",
        "The `dtype` property lets us see the data type of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlF3k3eUT_hQ",
        "outputId": "630cfa07-cef4-4273-9d36-21bb9625f3aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
        "x = torch.ones(3, 2)\n",
        "x.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1vtN4Dy8FAG"
      },
      "source": [
        "#### Shape\n",
        "\n",
        "The `shape` property tells us the shape of our tensor. This can help us identify how many dimensional our tensor is as well as how many elements exist in each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24gXLJcn7Pxs",
        "outputId": "719ec1bc-a418-442d-d0f1-f707e4a9f049"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV0cE1cXAEHP",
        "outputId": "6ecab05c-35e7-460b-c6a6-6921fd4bf234"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out its shape\n",
        "# Same as x.size()\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vnJnaAQlc",
        "outputId": "efd715a4-4b83-475e-baf4-c23ce59d227f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out the number of elements in a particular dimension\n",
        "# 0th dimension corresponds to the rows\n",
        "x.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8u_Rp5opra5",
        "outputId": "9a79ea8a-54c0-4bd1-e201-91b57960ab6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxXCX6y6BvhH"
      },
      "source": [
        "We can also get the size of a particular dimension with the `size()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZapQmydxBVuy",
        "outputId": "d8210596-beba-4fed-e0a0-4271448275a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the size of the 0th dimension\n",
        "x.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnxQ8aENp3QL",
        "outputId": "f15be279-1f8a-4aca-e408-08e49acc956d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCQm7ToPOveH"
      },
      "source": [
        "We can change the shape of a tensor with the `view()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1JH3fiNO5Gu",
        "outputId": "976fbe57-aa45-4c42-8d06-ae49aebeab9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example use of view()\n",
        "# x_view shares the same memory as x, so changing one changes the other\n",
        "x_view = x.view(2, 3)\n",
        "x_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3x4seqPGEI",
        "outputId": "094983bc-1031-4d08-bc29-b9b6418cdd89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can ask PyTorch to infer the size of a dimension with -1\n",
        "x_view = x.view(3, -1)\n",
        "x_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYSCEesPITpf"
      },
      "source": [
        "We can also use `torch.reshape()` method for a similar purpose. There is a subtle difference between `reshape()` and `view()`: `view()` requires the data to be stored contiguously in the memory. You can refer to [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) StackOverflow answer for more information. In simple terms, contiguous means that the way our data is laid out in the memory is the same as the way we would read elements from it. This happens because some methods, such as `transpose()` and `view()`, do not actually change how our data is stored in the memory. They just change the meta information about out tensor, so that when we use it we will see the elements in the order we expect.\n",
        "\n",
        "`reshape()` calls `view()` internally if the data is stored contiguously, if not, it returns a copy. The difference here isn't too important for basic tensors, but if you perform operations that make the underlying storage of the data non-contiguous (such as taking a transpose), you will have issues using `view()`. If you would like to match the way your tensor is stored in the memory to how it is used, you can use the `contiguous()` method.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLGcGYE4Llom",
        "outputId": "3e8565b5-437c-4a6f-f067-7d49ca9e709d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Change the shape of x to be 3x2\n",
        "# x_reshaped could be a reference to or copy of x\n",
        "x_reshaped = torch.reshape(x, (2, 3))\n",
        "x_reshaped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNTZKZZQ9i6"
      },
      "source": [
        "We can use `torch.unsqueeze(x, dim)` function to add a dimension of size `1` to the provided `dim`, where `x` is the tensor. We can also use the corresponding use `torch.squeeze(x)`, which removes the dimensions of size `1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_IYojrJRh-m",
        "outputId": "2c25eaf4-f172-4bd7-e804-20442d582814"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5],\n",
              "        [6, 7],\n",
              "        [8, 9]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 5x2 tensor, with 5 rows and 2 columns\n",
        "x = torch.arange(10).reshape(5, 2)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLhg_oZ4SHh-",
        "outputId": "5c4f6ffb-e305-401d-be38-e28259fd79b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 2])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add a new dimension of size 1 at the 1st dimension\n",
        "x = x.unsqueeze(1)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoGYGbMRSo-J",
        "outputId": "a64d70f7-c9e3-4345-c0fd-56cca062975a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 2])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\n",
        "x = x.squeeze()\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpZ4556B3lb"
      },
      "source": [
        "If we want to get the total number of elements in a tensor, we can use the `numel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-irUWlxTB6a",
        "outputId": "94b263e1-6c19-478c-efd5-2144b11c67e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5],\n",
              "        [6, 7],\n",
              "        [8, 9]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76yVMMg_CA0Q",
        "outputId": "fc6e076d-a915-408f-a188-a7de7acad43b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the number of elements in tensor.\n",
        "x.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M1U_RTpBhl2"
      },
      "source": [
        "#### **Device**\n",
        "Device property tells `PyTorch` where to store our tensor. Where a tensor is stored determines which device, `GPU` or `CPU`, would be handling the computations involving it. We can find the device of a tensor with the `device` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYRGhIbnCl3b",
        "outputId": "756408bd-7492-477b-8c1a-e4858f72a5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.Tensor([[1, 2], [3, 4]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byEnJyKdBgjl",
        "outputId": "9ca811fa-c526-43e5-bd25-3d961dae7381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the device of the tensor\n",
        "x.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF_PESATrWcw"
      },
      "outputs": [],
      "source": [
        "y = torch.arange(4).reshape((2, 2))\n",
        "y = y.to('cuda')\n",
        "# y + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIY4dJzjrsUu",
        "outputId": "1e59be44-f621-4b9b-babf-d427d445c8b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "Mkpi_B2FsBlH",
        "outputId": "054a13af-c6a1-4e9f-8cf1-449886f8fef2"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-cd60f97aa77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "source": [
        "x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPaelCkTsChz"
      },
      "outputs": [],
      "source": [
        "x.cuda() + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F13kZMNrw7x"
      },
      "outputs": [],
      "source": [
        "x + y.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7BMktFFAkRA"
      },
      "source": [
        "### Tensor Indexing\n",
        "In `PyTorch` we can index tensors, similar to `NumPy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRJN7ovWDsKV"
      },
      "outputs": [],
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.Tensor([\n",
        "                  [[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]],\n",
        "                  [[9, 10], [11, 12]]\n",
        "                 ])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67ZiOF1Heyc"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guXKE7m8AX1K"
      },
      "outputs": [],
      "source": [
        "# Access the 0th element, which is the first row\n",
        "x[0] # Equivalent to x[0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8m8EyVvES4-"
      },
      "source": [
        "We can also index into multiple dimensions with `:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z6GFUcuEL85"
      },
      "outputs": [],
      "source": [
        "# Get the top left element of each element in our tensor\n",
        "x[:, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm8vc3nuXaEw"
      },
      "source": [
        "We can also access arbitrary elements in each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYhcH9gaWHyW"
      },
      "outputs": [],
      "source": [
        "# Print x again to see our tensor\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4xl6CW3RrEw"
      },
      "outputs": [],
      "source": [
        "# Let's access the 0th and 1st elements, each twice\n",
        "i = torch.tensor([0, 0, 1, 1])\n",
        "x[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3QYZ8k7Wvqp"
      },
      "outputs": [],
      "source": [
        "# Let's access the 0th elements of the 1st and 2nd elements\n",
        "i = torch.tensor([1, 2])\n",
        "j = torch.tensor([0])\n",
        "x[i, j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAELXC--IHS7"
      },
      "source": [
        "We can get a `Python` scalar value from a tensor with `item()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM-ZujN2IGaQ"
      },
      "outputs": [],
      "source": [
        "x[0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NwxK7d_Ycgs"
      },
      "outputs": [],
      "source": [
        "x[0, 0, 0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GltnmDzeIXJM"
      },
      "source": [
        "### Operations\n",
        "PyTorch operations are very similar to those of `NumPy`. We can work with both scalars and other tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9KBzcA0G6v9"
      },
      "outputs": [],
      "source": [
        "# Create an example tensor\n",
        "x = torch.ones((3, 2, 2))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUw8MAHqKuzs"
      },
      "outputs": [],
      "source": [
        "# Perform elementwise addition\n",
        "# Use - for subtraction\n",
        "x + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAfMsaz1Gw5v"
      },
      "outputs": [],
      "source": [
        "# Perform elementwise multiplication\n",
        "# Use / for division\n",
        "x * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aq89FU7OOe7"
      },
      "source": [
        "We can apply the same operations between different tensors of compatible sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGhz62wILIfN"
      },
      "outputs": [],
      "source": [
        "# Create a 4x3 tensor of 6s\n",
        "a = torch.ones((4,3)) * 6\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvDC1OzzPyLV"
      },
      "outputs": [],
      "source": [
        "# Create a 1D tensor of 2s\n",
        "b = torch.ones(3) * 2\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUF9noMTP5NI"
      },
      "outputs": [],
      "source": [
        "# Divide a by b\n",
        "a / b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRZC1gAhuZNW"
      },
      "outputs": [],
      "source": [
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTiVVbukXRct"
      },
      "source": [
        "We can use `tensor.matmul(other_tensor)` for matrix multiplication and `tensor.T` for transpose. Matrix multiplication can also be performed with `@`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPoC_WcbXCw5"
      },
      "outputs": [],
      "source": [
        "# Alternative to a.matmul(b)\n",
        "# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n",
        "# is inferred\n",
        "(a @ b).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PibNpxbYYf2"
      },
      "source": [
        "We can take the mean and standard deviation along a certain dimension with the methods `mean(dim)` and `std(dim)`. That is, if we want to get the mean `3x2` matrix in a `4x3x2` matrix, we would set the `dim` to be 0. We can call these methods with no parameter to get the mean and standard deviation for the whole tensor. To use `mean` and `std` our tensor should be a floating point type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a987teCtYg7R"
      },
      "outputs": [],
      "source": [
        "# Create an example tensor\n",
        "m = torch.tensor(\n",
        "    [\n",
        "     [1., 1.],\n",
        "     [2., 2.],\n",
        "     [3., 3.],\n",
        "     [4., 4.]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Mean: {}\".format(m.mean()))\n",
        "print(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n",
        "print(\"Mean in the 1st dimension: {}\".format(m.mean(1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54tEhWJX4Kde"
      },
      "outputs": [],
      "source": [
        "print(\"Standard deviation:\", m.std().item())\n",
        "print(\"Median:\", m.median().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd77stQ5VQVT"
      },
      "source": [
        "We can concatenate tensors using `torch.cat`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advfDCOPK9Gw"
      },
      "outputs": [],
      "source": [
        "# Concatenate in dimension 0 and 1\n",
        "a_cat0 = torch.cat([a, a, a], dim=0)\n",
        "a_cat1 = torch.cat([a, a, a], dim=1)\n",
        "\n",
        "print(\"Initial shape: {}\".format(a.shape))\n",
        "print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n",
        "print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BveswZMOjtff"
      },
      "source": [
        "Most of the operations in `PyTorch` are not in place. However, `PyTorch` offers the in place versions of operations available by adding an underscore (`_`) at the end of the method name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebr7nn-DaU3B"
      },
      "outputs": [],
      "source": [
        "# Print our tensor\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP8-VtoHaKAc"
      },
      "outputs": [],
      "source": [
        "# add() is not in place\n",
        "a.add(a)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY0ojINbaayp"
      },
      "outputs": [],
      "source": [
        "# add_() is in place\n",
        "a.add_(a)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re8xiL37eAja"
      },
      "source": [
        "## Autograd\n",
        "`PyTorch` and other machine learning libraries are known for their automatic differantiation feature. That is, given that we have defined the set of operations that need to be performed, the framework itself can figure out how to compute the gradients. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oEvBJHWfn8H"
      },
      "outputs": [],
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTJazZXkgthP"
      },
      "outputs": [],
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "x.grad # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hqc2oM3iV6a"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K--Az0Xiic_z"
      },
      "outputs": [],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjPkiE6i7ja"
      },
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 拓展内容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYLWqKIoaOyd"
      },
      "source": [
        "## Neural Network Module\n",
        "\n",
        "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUmrDpbhV4Tn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGvRWjEbak0"
      },
      "source": [
        "### **Linear Layer**\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XfnKI4-a5j9"
      },
      "outputs": [],
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "# N* H_in -> N*H_out\n",
        "\n",
        "\n",
        "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
        "# dimensional outputs\n",
        "linear = nn.Linear(4, 2)\n",
        "nn.Linear(2,1)\n",
        "linear_output = linear(input)\n",
        "linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_9XKtAFYpdI"
      },
      "outputs": [],
      "source": [
        "list(linear.parameters()) # Ax + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXCCu9keUlW"
      },
      "source": [
        "### **Other Module Layers**\n",
        "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yslDOK66fYWn"
      },
      "source": [
        "### **Activation Function Layer**\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrJP5CveeOON"
      },
      "outputs": [],
      "source": [
        "linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9v5FjQtd4Ck"
      },
      "outputs": [],
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBwDgKxU5LD3"
      },
      "source": [
        "## Functional form\n",
        "\n",
        "Activation functions can be applied as layers (like with `nn.Sigmoid`) or using the functional form via `torch.nn.functional`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6uAEjMW5XKF"
      },
      "outputs": [],
      "source": [
        "torch.nn.functional.sigmoid(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiYTthJwhEYT"
      },
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequential`, which does exactly that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtJeOqLxhBLY"
      },
      "outputs": [],
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkJ81p3GUVPM"
      },
      "source": [
        "### Custom Modules\n",
        "\n",
        "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n",
        "\n",
        "To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
        "\n",
        "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2P7eZiMj32_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super().__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining of our model\n",
        "    # There isn't anything specific about the naming of `self.model`. It could\n",
        "    # be something arbitrary.\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input_size, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, self.input_size),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2DrfLiBVjNT"
      },
      "source": [
        "Here is an alternative way to define the same class. You can see that we can replace `nn.Sequential` by defining the individual layers in the `__init__` method and connecting the in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-lqhsqwViIk"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining of our layers\n",
        "    self.linear = nn.Linear(self.input_size, self.hidden_size) # Ax + b ==> A.shape? b.shape? (go from size 5 to 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(self.hidden_size, self.input_size) # Ax + b ==> A.shape? b.shape? (go from size 3 to 5)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear = self.linear(x)\n",
        "    relu = self.relu(linear)\n",
        "    linear2 = self.linear2(relu)\n",
        "    output = self.sigmoid(linear2)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEiLs_02HIHT"
      },
      "source": [
        "<i><b style=\"color: red\">Question: How many parameters in this model?</b></i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQelcFo5bXgU"
      },
      "source": [
        "Now that we have defined our class, we can instantiate it and see what it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXi0T0FZbV0y",
        "outputId": "315f82ae-f796-4e33-ee78-47fe28b1d3b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5510, 0.4670, 0.5036, 0.3651, 0.4176],\n",
              "        [0.3549, 0.4204, 0.4885, 0.4946, 0.4341]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Make a sample input\n",
        "input = torch.randn(2, 5)\n",
        "\n",
        "# Create our model\n",
        "model = MultilayerPerceptron(5, 3)\n",
        "\n",
        "# Pass our input through our model\n",
        "model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCCbjc-Fb2-B"
      },
      "source": [
        "We can inspect the parameters of our model with `named_parameters()` and `parameters()` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d23soYIb2WZ",
        "outputId": "d391313f-632b-4f6e-9004-3a88aed0663e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "linear.weight has 15 parameters\n",
            "linear.bias has 3 parameters\n",
            "linear2.weight has 15 parameters\n",
            "linear2.bias has 5 parameters\n",
            "----------------------------------------\n",
            "total num parameters: 38\n"
          ]
        }
      ],
      "source": [
        "num_parameters = 0\n",
        "for n, p in model.named_parameters():\n",
        "  print(n, 'has', p.numel(), 'parameters')\n",
        "  num_parameters += p.numel()\n",
        "print('-' * 40)\n",
        "print('total num parameters:', num_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXxVPI0nv5KI",
        "outputId": "372279e8-f810-4b16-8031-e48c106f2f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultilayerPerceptron(\n",
            "  (linear): Linear(in_features=5, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (linear2): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5JegycOdMFy"
      },
      "source": [
        "## Optimization\n",
        "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0F-TvV0kk-I"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgak6o5dlQWF"
      },
      "source": [
        "After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGYFiaT_vXBn",
        "outputId": "d8739815-2971-467e-d1fb-3eb1c22d11a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.2214,  0.6889,  0.5493,  1.2362,  1.9521],\n",
              "        [ 0.5472,  1.7272,  0.9165,  1.0015,  1.1384],\n",
              "        [-0.1626,  0.7759,  0.1111,  2.1401,  0.2645],\n",
              "        [ 1.4272,  1.7919,  1.3914,  0.4673,  1.6070],\n",
              "        [ 0.9705,  1.5223,  1.0252,  2.3606,  1.0656],\n",
              "        [ 2.2029,  0.7358, -0.3979,  2.0095, -0.7031],\n",
              "        [ 1.6055,  0.0042, -0.8418, -1.1823,  3.0799],\n",
              "        [ 0.5836,  1.7840,  0.8600,  1.2844,  1.0638],\n",
              "        [ 1.8731, -0.1323, -0.1827,  0.6417, -0.2170],\n",
              "        [ 0.8074,  1.4787,  0.6010,  0.4693, -0.0743]])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want out model to predict our original data, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEsiOdpWvfLj"
      },
      "source": [
        "Now, we can define our model, optimizer and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oA2XsdsbN8p",
        "outputId": "4bff7296-c1ca-4a36-f01f-22b6d4dc6631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6781231164932251"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = MultilayerPerceptron(5, 3)\n",
        "\n",
        "# Define the optimizer\n",
        "adam = optim.Adam(model.parameters(), lr=1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Calculate how our model is doing now\n",
        "y_pred = model(x)\n",
        "loss_function(y_pred, y).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxU7Y8ZufSR"
      },
      "source": [
        "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogl6-Ctmuek6",
        "outputId": "ac8395f9-2e4d-44ed-d076-bfccdb653c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: traing loss: 0.6781231164932251\n",
            "Epoch 1: traing loss: 0.26641646027565\n",
            "Epoch 2: traing loss: 0.11468765139579773\n",
            "Epoch 3: traing loss: 0.0518057644367218\n",
            "Epoch 4: traing loss: 0.025344399735331535\n",
            "Epoch 5: traing loss: 0.013451475650072098\n",
            "Epoch 6: traing loss: 0.007681891787797213\n",
            "Epoch 7: traing loss: 0.004674314986914396\n",
            "Epoch 8: traing loss: 0.0030038156546652317\n",
            "Epoch 9: traing loss: 0.002023308305069804\n"
          ]
        }
      ],
      "source": [
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: training loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  adam.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrMJ8AmqeCY-",
        "outputId": "395837a6-5bdc-4903-8c96-209762dd012a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-4.5308, -3.9733, -4.4686, -4.0116, -3.8651],\n",
              "         [-3.9764, -4.0669, -4.4570, -4.2890, -3.8520],\n",
              "         [-3.8711, -4.1119, -4.1699, -4.0015, -4.5175]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-4.3073, -4.4077, -4.2221], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[4.2747, 3.7099, 4.0836],\n",
              "         [3.8719, 4.4511, 3.6660],\n",
              "         [3.7980, 4.4729, 3.7731],\n",
              "         [4.3358, 3.7851, 4.6103],\n",
              "         [3.8353, 3.6951, 3.7058]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([6.5959, 6.7022, 6.5133, 6.4282, 6.5640], requires_grad=True)]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nXApd82wlsF"
      },
      "source": [
        "You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRqE7P9EtvuS",
        "outputId": "8e0a9aba-cced-425a-eb41-5c274b58ebd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See how our model performs on the training data\n",
        "y_pred = model(x)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJng31_Pi2R6",
        "outputId": "7cb3d853-77d9-4c9d-b2cb-05d51a0d47b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986],\n",
              "        [0.9986, 0.9988, 0.9985, 0.9984, 0.9986]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create test data and check how our model performs on it\n",
        "x2 = y + torch.randn_like(y)\n",
        "y_pred = model(x2)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WNk6oIZw2xo"
      },
      "source": [
        "Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
